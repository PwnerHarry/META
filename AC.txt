# TODO: implement actor-critic(\lambda)
# TODO: should be able to have APIs to the policy evaluation methods!
import gym, numpy as np
from utils import *
from greedy import *
from mta import *
from TOGTD import *
from TOTD import *
from scipy.special import softmax

def actor_critic(env, episodes, evaluate, encoder, critic_type='MTA', learner_type='togtd', gamma=lambda x: 0.95, alpha=0.05, beta=0.05):
    # suppose we use exponential softmax on values
    D = encoder(0).size
    theta, w = np.zeros((env.action_space.n, D)), np.zeros(D)
    e_theta, e_w = np.zeros(env.action_space.n), np.zeros(D) # TODO: if theta is now a matrix, how do we define e_theta?
    I = 1
    if learner_type == 'totd':
        LEARNER = TOTD_LEARNER
    elif learner_type == 'togtd':
        LEARNER = TOGTD_LEARNER
    if critic_type == 'baseline':
        learner = LEARNER(env, D)
    elif critic_type == 'greedy':
        first_moment_learner, variance_learner, value_learner = LEARNER(env, D), LEARNER(env, D), LEARNER(env, D)
    elif critic_type == 'MTA':
        MC_exp_learner, L_exp_learner, L_var_learner, value_learner = LEARNER(env, D), LEARNER(env, D), LEARNER(env, D), LEARNER(env, D)
    # TODO: RL Book 2018, pp. 354
    for episode in range(episodes):
        o_curr, done = env.reset(), False
        x_curr = encoder(o_curr)
        while not done:
            prob_actions = softmax(np.dot(theta, x_curr)) # https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html
            action = np.random.choice(range(len(prob_actions)), p=prob_actions)
            o_next, R_next, done, _ = env.step(action)
            x_next = encoder(o_next)
            # one-step of policy evaluation of the critic!
            if critic_type == 'baseline':
                if not done:
                    learner.learn(r_next, gamma(x_next), gamma(x_curr), x_next, x_curr, Lambda.value(x_next), Lambda.value(x_curr), rho_curr, alpha, beta)
                else:
                    learner.learn(r_next, 0, gamma(x_curr), x_next, x_curr, Lambda.value(x_next), Lambda.value(x_curr), rho_curr, alpha, beta)
            elif critic_type == 'greedy':
                if learner_type == 'togtd':
                    first_moment_learner.learn(R_next, gamma(x_next), gamma(x_curr), x_next, x_curr, 1.0, 1.0, rho_curr, alpha, beta)
                elif learner_type == 'totd':
                    first_moment_learner.learn(R_next, gamma(x_next), gamma(x_curr), x_next, x_curr, 1.0, 1.0, rho_curr, alpha)
                if not done:
                    delta_curr = R_next + gamma(x_next) * np.dot(x_next, value_learner.w_curr) - np.dot(x_curr, value_learner.w_curr)
                else:
                    delta_curr = R_next - np.dot(x_curr, value_learner.w_curr)
                r_bar_next = delta_curr ** 2
                gamma_bar_next = (rho_curr * gamma(x_next)) ** 2
                if learner_type == 'togtd':
                    variance_learner.learn(r_bar_next, gamma_bar_next, 1, x_next, x_curr, 1, 1, 1, alpha, beta)
                elif learner_type == 'totd':
                    variance_learner.learn(r_bar_next, gamma_bar_next, 1, x_next, x_curr, 1, 1, 1, alpha)
                errsq = (np.dot(x_next, first_moment_learner.w_next) - np.dot(x_next, value_learner.w_curr)) ** 2
                varg = max(0, np.dot(x_next, variance_learner.w_next))
                if errsq + varg > 0:
                    Lambda.w[o_next] = errsq / (errsq + varg)
                else:
                    Lambda.w[o_next] = 1
                if learner_type == 'togtd':
                    value_learner.learn(R_next, gamma(x_next), gamma(x_curr), x_next, x_curr, Lambda.w[o_next], Lambda.w[o_curr], rho_curr, alpha, beta)
                elif learner_type == 'totd':
                    value_learner.learn(R_next, gamma(x_next), gamma(x_curr), x_next, x_curr, Lambda.w[o_next], Lambda.w[o_curr], rho_curr, alpha)
            elif critic_type == 'MTA':
                if learner_type == 'togtd':
                    MC_exp_learner.learn(R_next, gamma(x_next), gamma(x_curr), x_next, x_curr, 1.0, 1.0, rho_curr, alpha, beta)
                    L_exp_learner.learn(R_next, gamma(x_next), gamma(x_curr), x_next, x_curr, Lambda.value(x_next), Lambda.value(x_curr), rho_curr, 1.1 * alpha, 1.1 * beta)
                elif learner_type == 'totd':
                    MC_exp_learner.learn(R_next, gamma(x_next), gamma(x_curr), x_next, x_curr, 1.0, 1.0, rho_curr, alpha)
                    L_exp_learner.learn(R_next, gamma(x_next), gamma(x_curr), x_next, x_curr, Lambda.value(x_next), Lambda.value(x_curr), rho_curr, 1.1 * alpha)
                if not done:
                    delta_curr = R_next + gamma(x_next) * np.dot(x_next, value_learner.w_curr) - np.dot(x_curr, value_learner.w_curr)
                else:
                    delta_curr = R_next - np.dot(x_curr, value_learner.w_curr)
                r_bar_next = delta_curr ** 2
                gamma_bar_next = (Lambda.value(x_next) * gamma(x_next)) ** 2
                if learner_type == 'togtd':
                    L_var_learner.learn(r_bar_next, gamma_bar_next, 1, x_next, x_curr, 1, 1, rho_curr, alpha, beta)
                elif learner_type == 'totd':
                    L_var_learner.learn(r_bar_next, gamma_bar_next, 1, x_next, x_curr, 1, 1, rho_curr, alpha)
                # SGD on meta-objective
                rho_acc = np.exp(log_rho_accu)
                # if rho_acc > 1e6: break # too much, not trustworthy
                v_next = np.dot(x_next, value_learner.w_curr)
                var_L_next, exp_L_next, exp_MC_next = np.dot(x_next, L_var_learner.w_curr), np.dot(x_next, L_exp_learner.w_curr), np.dot(x_next, MC_exp_learner.w_curr)
                coefficient = gamma(x_next) ** 2 * Lambda.value(x_next) * ((v_next - exp_L_next) ** 2 + var_L_next) + v_next * (exp_L_next + exp_MC_next) - v_next ** 2 - exp_L_next * exp_MC_next
                Lambda.gradient_descent(x_next, kappa * rho_acc * coefficient)
                # learn value
                if learner_type == 'togtd':
                    value_learner.learn(R_next, gamma(x_next), gamma(x_curr), x_next, x_curr, Lambda.value(x_next), Lambda.value(x_curr), rho_curr, alpha, beta)
                elif learner_type == 'totd':
                    value_learner.learn(R_next, gamma(x_next), gamma(x_curr), x_next, x_curr, Lambda.value(x_next), Lambda.value(x_curr), rho_curr, alpha)
            # one-step of policy improvement of the actor!
            e_theta = gamma * lambda_theta * e_theta + I * grad(np.log(behavior))# TODO: to intepret
            w += alpha_w + delta * e_w
            theta += alpha_theta + delta * e_theta]
            I *= gamma
            x_curr, o_curr = x_next, o_next            
    
    # TODO: to implement the main thing here.